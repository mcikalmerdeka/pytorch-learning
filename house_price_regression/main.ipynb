{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializations and Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the project root (assuming notebook is in a subfolder of the project)\n",
        "notebook_dir = Path().resolve()\n",
        "project_root = notebook_dir.parent  # Adjust .parent depth based on your structure\n",
        "\n",
        "# Add to path if not already there\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "\n",
        "Let's start by getting all our imports. Keep in mind that PyTorch does not automatically detect and train on GPU, you have to tell it to use cuda. In case you want to train on Mac Silicon, replace cuda with mps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam  # Adam Optimizer\n",
        "from torch.utils.data import Dataset, DataLoader  # Dataset class and DataLoader for creating the objects\n",
        "from torchinfo import summary  # Visualize the model layers and number of parameters\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # Split the dataset (train, validation, test)\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # Regression metrics\n",
        "import matplotlib.pyplot as plt  # Plotting the training progress at the end\n",
        "import pandas as pd  # Data reading and preprocessing\n",
        "import numpy as np  # Mathematical operations\n",
        "import seaborn as sns\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA & Preprocessing\n",
        "\n",
        "In this section, you'll perform exploratory data analysis and preprocessing. This includes:\n",
        "- Loading the Indonesian house prices dataset\n",
        "- Checking for missing values and handling them\n",
        "- Analyzing the distribution of the target variable (price_usd)\n",
        "- Exploring correlations between features\n",
        "- Handling categorical variables (encoding)\n",
        "- Detecting and handling outliers\n",
        "- Feature scaling/normalization\n",
        "\n",
        "Key things to consider for regression:\n",
        "- The target variable (price_usd) is continuous, not categorical\n",
        "- You'll need to encode categorical features like city, district, property_type, etc.\n",
        "- Consider removing or encoding the listing_date column\n",
        "- Check the distribution of prices - you might want to use log transformation if it's heavily skewed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset overview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>listing_date</th>\n",
              "      <th>city</th>\n",
              "      <th>district</th>\n",
              "      <th>property_type</th>\n",
              "      <th>price_idr</th>\n",
              "      <th>price_usd</th>\n",
              "      <th>land_size_sqm</th>\n",
              "      <th>building_size_sqm</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>floors</th>\n",
              "      <th>building_age_years</th>\n",
              "      <th>carports</th>\n",
              "      <th>certificate_type</th>\n",
              "      <th>furnishing</th>\n",
              "      <th>has_swimming_pool</th>\n",
              "      <th>has_garden</th>\n",
              "      <th>has_security</th>\n",
              "      <th>distance_to_city_center_km</th>\n",
              "      <th>nearby_facilities_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-03-13</td>\n",
              "      <td>Palembang</td>\n",
              "      <td>East</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>960365122</td>\n",
              "      <td>61170</td>\n",
              "      <td>117</td>\n",
              "      <td>117</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>SHM (Freehold)</td>\n",
              "      <td>Unfurnished</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14.3</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>Medan</td>\n",
              "      <td>Central</td>\n",
              "      <td>House</td>\n",
              "      <td>3685078430</td>\n",
              "      <td>234718</td>\n",
              "      <td>149</td>\n",
              "      <td>114</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>SHM (Freehold)</td>\n",
              "      <td>Unfurnished</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-18</td>\n",
              "      <td>Bogor</td>\n",
              "      <td>Central</td>\n",
              "      <td>Townhouse</td>\n",
              "      <td>1782086831</td>\n",
              "      <td>113509</td>\n",
              "      <td>123</td>\n",
              "      <td>82</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>HGB (Building Rights)</td>\n",
              "      <td>Semi-Furnished</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-07-09</td>\n",
              "      <td>Batam</td>\n",
              "      <td>North</td>\n",
              "      <td>House</td>\n",
              "      <td>4773459488</td>\n",
              "      <td>304042</td>\n",
              "      <td>507</td>\n",
              "      <td>213</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>SHM (Freehold)</td>\n",
              "      <td>Semi-Furnished</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14.1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-01-02</td>\n",
              "      <td>Bali (Denpasar)</td>\n",
              "      <td>West</td>\n",
              "      <td>Townhouse</td>\n",
              "      <td>3407557763</td>\n",
              "      <td>217042</td>\n",
              "      <td>385</td>\n",
              "      <td>246</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>HGB (Building Rights)</td>\n",
              "      <td>Semi-Furnished</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  listing_date             city district property_type   price_idr  price_usd  \\\n",
              "0   2025-03-13        Palembang     East     Apartment   960365122      61170   \n",
              "1   2025-08-07            Medan  Central         House  3685078430     234718   \n",
              "2   2025-09-18            Bogor  Central     Townhouse  1782086831     113509   \n",
              "3   2024-07-09            Batam    North         House  4773459488     304042   \n",
              "4   2025-01-02  Bali (Denpasar)     West     Townhouse  3407557763     217042   \n",
              "\n",
              "   land_size_sqm  building_size_sqm  bedrooms  bathrooms  floors  \\\n",
              "0            117                117         2          1       2   \n",
              "1            149                114         2          1       1   \n",
              "2            123                 82         3          3       2   \n",
              "3            507                213         2          1       1   \n",
              "4            385                246         6          6       1   \n",
              "\n",
              "   building_age_years  carports       certificate_type      furnishing  \\\n",
              "0                  13         0         SHM (Freehold)     Unfurnished   \n",
              "1                   2         1         SHM (Freehold)     Unfurnished   \n",
              "2                  10         1  HGB (Building Rights)  Semi-Furnished   \n",
              "3                  12         1         SHM (Freehold)  Semi-Furnished   \n",
              "4                  28         0  HGB (Building Rights)  Semi-Furnished   \n",
              "\n",
              "   has_swimming_pool  has_garden  has_security  distance_to_city_center_km  \\\n",
              "0                  0           1             1                        14.3   \n",
              "1                  0           0             1                         1.8   \n",
              "2                  1           1             1                         2.4   \n",
              "3                  0           1             1                        14.1   \n",
              "4                  0           0             1                        10.2   \n",
              "\n",
              "   nearby_facilities_count  \n",
              "0                        8  \n",
              "1                        1  \n",
              "2                        5  \n",
              "3                        1  \n",
              "4                        9  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Original data shape: (50000, 20)\n",
            "\n",
            "Column names and types:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "listing_date                   object\n",
              "city                           object\n",
              "district                       object\n",
              "property_type                  object\n",
              "price_idr                       int64\n",
              "price_usd                       int64\n",
              "land_size_sqm                   int64\n",
              "building_size_sqm               int64\n",
              "bedrooms                        int64\n",
              "bathrooms                       int64\n",
              "floors                          int64\n",
              "building_age_years              int64\n",
              "carports                        int64\n",
              "certificate_type               object\n",
              "furnishing                     object\n",
              "has_swimming_pool               int64\n",
              "has_garden                      int64\n",
              "has_security                    int64\n",
              "distance_to_city_center_km    float64\n",
              "nearby_facilities_count         int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "listing_date                  0\n",
              "city                          0\n",
              "district                      0\n",
              "property_type                 0\n",
              "price_idr                     0\n",
              "price_usd                     0\n",
              "land_size_sqm                 0\n",
              "building_size_sqm             0\n",
              "bedrooms                      0\n",
              "bathrooms                     0\n",
              "floors                        0\n",
              "building_age_years            0\n",
              "carports                      0\n",
              "certificate_type              0\n",
              "furnishing                    0\n",
              "has_swimming_pool             0\n",
              "has_garden                    0\n",
              "has_security                  0\n",
              "distance_to_city_center_km    0\n",
              "nearby_facilities_count       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Read dataset\n",
        "raw_df = pd.read_csv(\"indonesian_house_prices.csv\")\n",
        "df = raw_df.copy()\n",
        "print(\"Dataset overview:\")\n",
        "display(df.head())\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"\\nOriginal data shape: {df.shape}\")\n",
        "print(f\"\\nColumn names and types:\")\n",
        "display(df.dtypes)\n",
        "print(f\"\\nMissing values:\")\n",
        "display(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Add your preprocessing steps here\n",
        "# Examples:\n",
        "# - Drop unnecessary columns (id, listing_date, price_idr if using price_usd)\n",
        "# - Handle missing values\n",
        "# - Encode categorical variables\n",
        "# - Handle outliers\n",
        "# - Feature scaling\n",
        "\n",
        "# Placeholder: After preprocessing, you should have X and y ready\n",
        "# X should be your features (all columns except target)\n",
        "# y should be your target variable (price_usd or log-transformed price)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Splitting\n",
        "\n",
        "We will detect the inputs and the outputs of the data which are X and y respectively.\n",
        "\n",
        "Then we will split our data into the following:\n",
        "\n",
        "- Training Size 70%\n",
        "- Validation Size 15%\n",
        "- Testing Size 15%\n",
        "\n",
        "We will do this by splitting our data twice using the train_test_split function in sklearn. The function takes inputs, outputs and the testing size. After that we will print the training, validation and testing shapes and sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANT: After your preprocessing, replace these lines with your actual X and y\n",
        "# For now, this is a placeholder showing the expected structure\n",
        "\n",
        "# Example (replace with your preprocessed data):\n",
        "# X = df_scaled.drop(columns=[\"price_usd\"])  # or however you've named your target\n",
        "# y = df_scaled[\"price_usd\"]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)  # Replace with your feature matrix\n",
        "y = np.array(y)  # Replace with your target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the test dataset into test and validation\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# Display the shapes of the datasets\n",
        "print(f\"Training set is: {X_train.shape[0]} rows which is {round(X_train.shape[0]/(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),4)*100}%\")\n",
        "print(f\"Validation set is: {X_val.shape[0]} rows which is {round(X_val.shape[0]/(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),4)*100}%\")\n",
        "print(f\"Testing set is: {X_test.shape[0]} rows which is {round(X_test.shape[0]/(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),4)*100}%\")\n",
        "\n",
        "print(f\"\\nFeature dimensions: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Object\n",
        "\n",
        "Now we will create the dataset object. This part is not complex but it's a bit tricky. We need to convert our dataset to PyTorch Dataset object as it will be more efficient during training. You can use the dataset as is, but let's keep things professional and efficient. \n",
        "\n",
        "First we define our class that will be taking the main Dataset class with the concept of inheritance. There is a big class in PyTorch, this class has several functions inside it, we will recreate that class and modify some functions to match our needs.\n",
        "\n",
        "In the cell below, we rebuilt the constructor function which is `__init__`. We put X and y as parameters to this function which are the inputs and outputs respectively, then inside the function we define the inputs and convert them to tensors, then convert the outputs to tensors and we make the numbers as `float32`. Additionally, we moved all our data to the cuda device. Then we modified the `__len__` and the `__getitem__` to match our needs which gets the specific length/shape of the data, and the data of specific row in our data respectively.\n",
        "\n",
        "**Important difference from classification**: For regression, we don't need to convert y to long integers - we keep it as float32 since we're predicting continuous values, not classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the dataset object for regression\n",
        "class RegressionDataset(Dataset):\n",
        "\n",
        "    # Constructor function\n",
        "    def __init__(self, X, y):\n",
        "        # Convert to tensors (pytorch only takes tensors) and move to device\n",
        "        self.X = torch.tensor(X, dtype=torch.float32).to(device) \n",
        "        # For regression, y should be float32, not long (unlike classification)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).to(device) \n",
        "    \n",
        "    # Get the length of the data\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # Get the data of specific index\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "    \n",
        "# Create object for training, validation and testing\n",
        "training_data = RegressionDataset(X_train, y_train)\n",
        "validation_data = RegressionDataset(X_val, y_val)\n",
        "testing_data = RegressionDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Hyperparameters\n",
        "\n",
        "Now we are setting the training hyperparameters. We define some variables which are the batch size, number of training epochs, hidden neurons, and learning rate.\n",
        "\n",
        "For regression tasks, you might want to experiment with:\n",
        "- Different numbers of hidden layers\n",
        "- Different activation functions (ReLU is common for hidden layers)\n",
        "- Learning rate scheduling\n",
        "- Different optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "HIDDEN_NEURONS_1 = 64  # First hidden layer\n",
        "HIDDEN_NEURONS_2 = 32  # Second hidden layer\n",
        "LR = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loaders\n",
        "\n",
        "This concept may seem complicated, but it's very easy. PyTorch provides a class called DataLoader which allows us to create objects of it to simplify the training.\n",
        "\n",
        "DataLoader is an object that we can loop through it to train according to batches. When we start training, we loop through epochs. If you skip the batch size it means that the amount of training data in one batch is equal to the complete amount of training data, this method is not efficient and in most cases you need to train through using batches. DataLoader allows you to loop through the batches easily during the training. When you create a dataloader, you define the batch size and enable the shuffle to randomize the data and then you can loop through it in each epoch to train normally.\n",
        "\n",
        "In the next cell, we define a dataloader for each of our data (training, validation and testing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A dataloader is a class that loads the data in batches. \n",
        "# It's basically an object that we use to do the for loop during each epoch.\n",
        "\n",
        "# Create DataLoader objects\n",
        "training_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "testing_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of what the dataloader object looks like\n",
        "\n",
        "# The dataloader returns batches of data in each iteration\n",
        "# 'batch' contains the input features (X) for multiple samples\n",
        "# 'label' contains the corresponding target values (y) for those samples\n",
        "# Notice that each run of the for loop will return a different batch of data\n",
        "\n",
        "for batch, label in training_dataloader:\n",
        "    print(\"Batch shape (features):\", batch.shape)  # [batch_size, num_features]\n",
        "    print(\"First sample features:\", batch[0])  \n",
        "    \n",
        "    print(\"=\"*100)\n",
        "\n",
        "    print(\"Label shape (targets):\", label.shape)  # [batch_size]\n",
        "    print(\"First few target values (prices):\", label[:5])  # Show first 5 prices\n",
        "    break  # Only show one batch as an example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Class\n",
        "\n",
        "Creating a model in PyTorch seems not very straightforward in the beginning, but if you understand how machine learning and deep learning works, it will be easy for you to understand PyTorch structure easily.\n",
        "\n",
        "In the next cell we define a new class `RegressionModel` that inherits from `nn.Module` like we did for the dataset. Remember, in simple terms, we want to redefine some functions in the class to match our needs. In the constructor which is `__init__`, then `super(RegressionModel, self).__init__()` calls the constructor of the parent class nn.Module to ensure it's properly initialized.\n",
        "\n",
        "Then, we create our layers:\n",
        "- **Input layer**: Takes the input features and transforms them to the first hidden layer\n",
        "- **Hidden layers**: With ReLU activation for non-linearity\n",
        "- **Output layer**: Produces a single continuous value (the predicted price)\n",
        "\n",
        "**Key differences from classification:**\n",
        "1. **No Sigmoid/Softmax at the end**: For regression, we output a raw continuous value, not a probability\n",
        "2. **Output size is 1**: We're predicting a single continuous value (price)\n",
        "3. **ReLU activation**: Commonly used in hidden layers for regression tasks\n",
        "\n",
        "In the function `forward`, this function is the forward propagation of the model - how the data flows inside the model from the input to the output. This means we can control this completely. That's how PyTorch is so customizable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        # Call the parent class (nn.Module) constructor\n",
        "        # This initializes all the PyTorch machinery needed for your model\n",
        "        super(RegressionModel, self).__init__()\n",
        "\n",
        "        # First hidden layer: takes input features and transforms them\n",
        "        # input_size = number of input features (columns in your dataset)\n",
        "        # HIDDEN_NEURONS_1 = number of neurons in the first hidden layer\n",
        "        self.hidden1 = nn.Linear(input_size, HIDDEN_NEURONS_1)\n",
        "        \n",
        "        # Second hidden layer: takes output from first hidden layer\n",
        "        self.hidden2 = nn.Linear(HIDDEN_NEURONS_1, HIDDEN_NEURONS_2)\n",
        "        \n",
        "        # Output layer: produces a single continuous value (the predicted price)\n",
        "        # Unlike classification, we output 1 value, not a probability distribution\n",
        "        self.output = nn.Linear(HIDDEN_NEURONS_2, 1)\n",
        "        \n",
        "        # ReLU activation function for hidden layers\n",
        "        # ReLU(x) = max(0, x) - introduces non-linearity and prevents vanishing gradients\n",
        "        # This is the most common activation for regression tasks\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # Other activation options you could try for hidden layers:\n",
        "        # self.leaky_relu = nn.LeakyReLU()  # Allows small negative values\n",
        "        # self.elu = nn.ELU()  # Exponential Linear Unit, smoother than ReLU\n",
        "        # self.tanh = nn.Tanh()  # Output range [-1, 1]\n",
        "        # Note: For regression OUTPUT layer, we typically use NO activation (linear output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define how data flows through the network (forward pass)\n",
        "        # x starts as your input data (batch of samples with features)\n",
        "        \n",
        "        # Pass through first hidden layer and apply ReLU activation\n",
        "        x = self.relu(self.hidden1(x))\n",
        "        \n",
        "        # Pass through second hidden layer and apply ReLU activation\n",
        "        x = self.relu(self.hidden2(x))\n",
        "        \n",
        "        # Pass through output layer (NO activation here for regression)\n",
        "        # This gives us a raw continuous value as the prediction\n",
        "        x = self.output(x)\n",
        "        \n",
        "        # Return the predicted value (house price)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Creation\n",
        "\n",
        "Now let's create our model and move it to the assigned device (cuda if you have GPU or the CPU if you don't have any GPUs). Additionally, we will print a `summary` of the model using the summary function which will take our model and the input size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the neural network model\n",
        "# RegressionModel() creates a new instance of our custom neural network class\n",
        "# We pass the input size (number of features) to the constructor\n",
        "# .to(device) moves the model to the appropriate device (GPU if available, otherwise CPU)\n",
        "# This ensures all model parameters and computations happen on the correct hardware\n",
        "model = RegressionModel(input_size=X_train.shape[1]).to(device)\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "# torchinfo.summary() prints layer-by-layer details including:\n",
        "# - Layer types and names\n",
        "# - Output shapes at each layer\n",
        "# - Number of trainable parameters (weights and biases)\n",
        "# input_size specifies the input shape: a tuple with (batch_size, num_features)\n",
        "# This helps verify the model structure and parameter count before training\n",
        "summary(model, input_size=(1, X_train.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss and Optimizer\n",
        "\n",
        "In the next cell, we will create the loss function and the optimizer.\n",
        "\n",
        "**Key difference from classification:**\n",
        "- For **classification**, we used Binary Cross Entropy (BCE) loss\n",
        "- For **regression**, we use Mean Squared Error (MSE) loss, which measures the average squared difference between predictions and actual values\n",
        "\n",
        "The optimizer `Adam` will take the model parameters/weights and the learning rate, same as classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the loss function for regression\n",
        "# MSELoss (Mean Squared Error Loss) calculates the average of squared differences\n",
        "# between predictions and actual values: loss = mean((prediction - actual)^2)\n",
        "# This penalizes larger errors more heavily than smaller ones\n",
        "# \n",
        "# Other regression loss options:\n",
        "# - nn.L1Loss() - Mean Absolute Error, less sensitive to outliers\n",
        "# - nn.SmoothL1Loss() - Huber loss, combines MSE and MAE benefits\n",
        "# - nn.HuberLoss() - Similar to SmoothL1Loss with configurable delta\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer to update model weights during training\n",
        "# Adam (Adaptive Moment Estimation) adjusts learning rates automatically for each parameter\n",
        "# model.parameters() gives all trainable weights/biases to optimize\n",
        "# lr (learning rate) controls how big the weight updates are each step\n",
        "# Other optimizer options:\n",
        "# - torch.optim.SGD() - basic stochastic gradient descent, simpler but may need tuning\n",
        "# - torch.optim.AdamW() - Adam with weight decay, better generalization\n",
        "# - torch.optim.RMSprop() - good for RNNs\n",
        "optimizer = Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Now comes the exciting part. As we mentioned several times, nothing is complex here if you know how deep learning works. You just need to know PyTorch syntax. We start by initializing the for loop with the specified number of epochs. But before that we define 4 lists and inside the loop we define another 4 variables.\n",
        "\n",
        "### Variables:\n",
        "* `total_loss_train`: here we will keep track of the training loss progress during each epoch, we calculate the loss during the batch and we use the loss value to optimize and modify the model parameters.\n",
        "\n",
        "* `total_loss_val`: here we will keep track of the validation loss progress during each epoch, we calculate the loss during the batch to monitor for overfitting.\n",
        "\n",
        "### Lists:\n",
        "* `total_loss_train_plot`: We append the losses of the training to visualize them at the end.\n",
        "\n",
        "* `total_loss_validation_plot`: We append the losses of the validation to visualize them at the end.\n",
        "\n",
        "**Key differences from classification:**\n",
        "1. We don't track accuracy (that's for classification). For regression, loss (MSE) is our main metric.\n",
        "2. We can also track additional metrics like MAE (Mean Absolute Error) or R² score for better understanding.\n",
        "3. No rounding or class comparison - we directly compare continuous predicted values with actual values.\n",
        "\n",
        "### Training Process\n",
        "Then, we start to loop through the training dataloaders. In the loop, we get our data from the data loader, and the inputs and labels are already on the device. We allow the model to make a prediction (forward propagation), then we get the output of the model and compare it with our original output using the loss criterion. We use the squeeze function to remove unnecessary dimensions. We add the loss amount to `total_loss_train`. \n",
        "\n",
        "Then we do `batch_loss.backward()` which performs backpropagation, we use the optimizer to update the weights using `optimizer.step()`, and then we reset the optimizer gradients using `optimizer.zero_grad()` which is a very important step.\n",
        "\n",
        "### Validation\n",
        "After that we exit the batch loop (train dataloader loop) and we start with the validation. Don't forget that we are still in the same epoch. Inside that we start with `torch.no_grad()` which means that we need the model to do predictions without being trained. We just need to see the validation performance. Then we do the same steps which are predicting and calculating loss.\n",
        "\n",
        "At the end we print after each epoch the epoch number, training loss, and validation loss. We use the printing of \"=\" signs just for making the printing output look clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize empty lists to store metrics for each epoch (used for plotting later)\n",
        "total_loss_train_plot = []  # Stores training loss per epoch\n",
        "total_loss_validation_plot = []  # Stores validation loss per epoch\n",
        "\n",
        "# Main training loop - runs for the specified number of epochs\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset metrics at the start of each epoch\n",
        "    total_loss_train = 0  # Accumulates loss during training\n",
        "    total_loss_val = 0  # Accumulates loss during validation\n",
        "    \n",
        "    # Set model to training mode (enables dropout, batch norm updates, etc.)\n",
        "    model.train()\n",
        "\n",
        "    ## Training\n",
        "    # Loop through all batches in the training dataset\n",
        "    for data in training_dataloader:\n",
        "\n",
        "        # Unpack the batch into inputs (features) and labels (target prices)\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Forward pass: feed inputs through the model to get predictions\n",
        "        # squeeze(1) removes dimension 1 to match label shape (e.g., [64,1] -> [64])\n",
        "        prediction = model(inputs).squeeze(1)\n",
        "\n",
        "        # Calculate loss: Mean Squared Error between predictions and actual prices\n",
        "        batch_loss = criterion(prediction, labels)\n",
        "\n",
        "        # Add this batch's loss to the running total\n",
        "        # .item() converts the tensor to a Python number\n",
        "        total_loss_train += batch_loss.item()\n",
        "\n",
        "        # Backward pass: calculate gradients (how to adjust weights)\n",
        "        batch_loss.backward()\n",
        "        \n",
        "        # Update model weights based on calculated gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Clear gradients for next iteration (PyTorch accumulates them by default)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    ## Validation\n",
        "    # Set model to evaluation mode (disables dropout, batch norm in eval mode, etc.)\n",
        "    model.eval()\n",
        "    \n",
        "    # Disable gradient calculation for validation (saves memory, speeds up computation)\n",
        "    with torch.no_grad():\n",
        "        # Loop through all batches in the validation dataset\n",
        "        for data in validation_dataloader:\n",
        "            # Unpack batch\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Forward pass: get predictions (same as training)\n",
        "            prediction = model(inputs).squeeze(1)\n",
        "\n",
        "            # Calculate loss (same as training)\n",
        "            batch_loss = criterion(prediction, labels)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            total_loss_val += batch_loss.item()\n",
        "\n",
        "    # After both training and validation, store metrics for this epoch\n",
        "    # Calculate average loss per batch\n",
        "    avg_train_loss = total_loss_train / len(training_dataloader)\n",
        "    avg_val_loss = total_loss_val / len(validation_dataloader)\n",
        "    \n",
        "    total_loss_train_plot.append(avg_train_loss)\n",
        "    total_loss_validation_plot.append(avg_val_loss)\n",
        "\n",
        "    # Print epoch summary: shows progress of training and validation\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Epoch {epoch + 1:3d}\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"  Train      - Loss (MSE): {avg_train_loss:12.4f}\")\n",
        "    print(f\"  Validation - Loss (MSE): {avg_val_loss:12.4f}\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing\n",
        "\n",
        "Now in this section, we will be testing our model. We will start the code with `torch.no_grad()` which means that we are telling PyTorch that we don't want to train the model, we will be using it only for testing. Then we will declare initial loss as zero and empty lists for predictions and actual values.\n",
        "\n",
        "We will start by looping through the testing dataloader like we did before during training. Inside the loop, we get our data and we run our model on the data to get the predictions. We calculate the loss and add it to our overall loss. We also collect predictions and actual values to calculate additional regression metrics.\n",
        "\n",
        "**Key differences from classification:**\n",
        "- Instead of accuracy, we calculate regression metrics: MSE, RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), MAPE (Mean Absolute Percentage Error), and R² score\n",
        "- R² score tells us the proportion of variance explained by our model (1.0 is perfect, 0.0 means the model is no better than predicting the mean)\n",
        "- MAPE gives us the average percentage error, making it easy to interpret (e.g., 10% means predictions are off by 10% on average)\n",
        "- We compare continuous predicted values directly with actual values, no rounding needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for testing (no backpropagation needed)\n",
        "with torch.no_grad():\n",
        "    # Initialize testing metrics\n",
        "    total_loss_test = 0\n",
        "    all_predictions = []  # Store all predictions\n",
        "    all_actuals = []  # Store all actual values\n",
        "    \n",
        "    # Loop through all batches in the testing dataset\n",
        "    for data in testing_dataloader:\n",
        "        # Unpack batch\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Forward pass: get predictions\n",
        "        prediction = model(inputs).squeeze(1)\n",
        "\n",
        "        # Calculate loss for this batch\n",
        "        batch_loss_test = criterion(prediction, labels)\n",
        "        \n",
        "        # Accumulate testing loss\n",
        "        total_loss_test += batch_loss_test.item()\n",
        "        \n",
        "        # Store predictions and actual values for metrics calculation\n",
        "        all_predictions.extend(prediction.cpu().numpy())\n",
        "        all_actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate average test loss\n",
        "avg_test_loss = total_loss_test / len(testing_dataloader)\n",
        "\n",
        "# Convert lists to numpy arrays for sklearn metrics\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_actuals = np.array(all_actuals)\n",
        "\n",
        "# Calculate regression metrics\n",
        "mse = mean_squared_error(all_actuals, all_predictions)\n",
        "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
        "mae = mean_absolute_error(all_actuals, all_predictions)  # Mean Absolute Error\n",
        "mape = np.mean(np.abs((all_actuals - all_predictions) / all_actuals)) * 100  # Mean Absolute Percentage Error\n",
        "r2 = r2_score(all_actuals, all_predictions)  # R² score\n",
        "\n",
        "# Print final testing metrics\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Mean Squared Error (MSE):        {mse:12.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE):  {rmse:12.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE):       {mae:12.4f}\")\n",
        "print(f\"Mean Absolute Percentage Error:  {mape:12.4f}%\")\n",
        "print(f\"R² Score:                        {r2:12.4f}\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"- On average, predictions are off by ${mae:,.2f} (MAE)\")\n",
        "print(f\"- On average, predictions are off by {mape:.2f}% (MAPE)\")\n",
        "print(f\"- The model explains {r2*100:.2f}% of the variance in house prices (R²)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Testing Analysis\n",
        "\n",
        "Let's create some visualizations to better understand our model's performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a figure with multiple subplots for analysis\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Predicted vs Actual values\n",
        "axes[0].scatter(all_actuals, all_predictions, alpha=0.5)\n",
        "axes[0].plot([all_actuals.min(), all_actuals.max()], \n",
        "             [all_actuals.min(), all_actuals.max()], \n",
        "             'r--', lw=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual Prices')\n",
        "axes[0].set_ylabel('Predicted Prices')\n",
        "axes[0].set_title('Predicted vs Actual House Prices')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residuals (errors) distribution\n",
        "residuals = all_actuals - all_predictions\n",
        "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
        "axes[1].set_xlabel('Residuals (Actual - Predicted)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Distribution of Prediction Errors')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nResidual Statistics:\")\n",
        "print(f\"Mean of residuals: {residuals.mean():.4f} (should be close to 0)\")\n",
        "print(f\"Std of residuals: {residuals.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Training Progress\n",
        "\n",
        "Let's visualize how the training and validation loss changed over epochs. This helps us understand if the model learned properly and if there's any overfitting.\n",
        "\n",
        "**What to look for:**\n",
        "- Both training and validation loss should decrease over time\n",
        "- If validation loss starts increasing while training loss decreases, that's overfitting\n",
        "- If both losses plateau, the model has converged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a plot for training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(total_loss_train_plot, label='Training Loss', linewidth=2)\n",
        "plt.plot(total_loss_validation_plot, label='Validation Loss', linewidth=2)\n",
        "plt.title('Training and Validation Loss over Epochs', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Trained Model\n",
        "\n",
        "Now let's save our trained model so we can use it later without retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model state dict (recommended approach)\n",
        "torch.save(model.state_dict(), 'house_price_regression_model.pth')\n",
        "\n",
        "# Optionally save additional information for reproducibility\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'input_size': X_train.shape[1],\n",
        "    'hidden_neurons_1': HIDDEN_NEURONS_1,\n",
        "    'hidden_neurons_2': HIDDEN_NEURONS_2,\n",
        "    'final_train_loss': total_loss_train_plot[-1],\n",
        "    'final_val_loss': total_loss_validation_plot[-1],\n",
        "    'test_mse': mse,\n",
        "    'test_r2': r2\n",
        "}, 'house_price_regression_checkpoint.pth')\n",
        "\n",
        "print(\"Model saved successfully!\")\n",
        "print(\"\\nFiles created:\")\n",
        "print(\"  - house_price_regression_model.pth (model weights only)\")\n",
        "print(\"  - house_price_regression_checkpoint.pth (full checkpoint with metadata)\")\n",
        "\n",
        "print(\"\\nTo load the model later:\")\n",
        "print(\"  model = RegressionModel(input_size=X.shape[1]).to(device)\")\n",
        "print(\"  model.load_state_dict(torch.load('house_price_regression_model.pth'))\")\n",
        "print(\"  model.eval()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference\n",
        "\n",
        "Now let's see how we can use the model in inference mode. Inference means making predictions like you are in production.\n",
        "\n",
        "**Important**: You need to preprocess your input data the same way you preprocessed your training data:\n",
        "- Apply the same scaling/normalization\n",
        "- Encode categorical variables the same way\n",
        "- Use the same feature order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Make a prediction for a new house\n",
        "# You need to prepare your input with the same preprocessing as training data\n",
        "\n",
        "# Let's use an example from the test set to demonstrate\n",
        "sample_idx = 0  # First sample from test set\n",
        "sample_features = X_test[sample_idx]\n",
        "actual_price = y_test[sample_idx]\n",
        "\n",
        "# Convert to tensor and add batch dimension\n",
        "model_input = torch.tensor(sample_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "# Make prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted_price = model(model_input).item()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INFERENCE EXAMPLE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Actual Price:    ${actual_price:,.2f}\")\n",
        "print(f\"Predicted Price: ${predicted_price:,.2f}\")\n",
        "print(f\"Difference:      ${abs(actual_price - predicted_price):,.2f}\")\n",
        "print(f\"Error Percentage: {abs(actual_price - predicted_price) / actual_price * 100:.2f}%\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For a completely new house, you would do something like this:\n",
        "# (This is pseudocode - adjust based on your actual preprocessing)\n",
        "\n",
        "# new_house = {\n",
        "#     'land_size_sqm': 200,\n",
        "#     'building_size_sqm': 150,\n",
        "#     'bedrooms': 3,\n",
        "#     'bathrooms': 2,\n",
        "#     'floors': 2,\n",
        "#     'building_age_years': 5,\n",
        "#     'carports': 1,\n",
        "#     'city': 'Jakarta',  # Need to encode\n",
        "#     'property_type': 'House',  # Need to encode\n",
        "#     # ... other features\n",
        "# }\n",
        "\n",
        "# Steps:\n",
        "# 1. Encode categorical variables (same encoding as training)\n",
        "# 2. Apply the same scaling/normalization (using the same scaler from training)\n",
        "# 3. Arrange features in the same order as training data\n",
        "# 4. Convert to tensor and make prediction\n",
        "\n",
        "print(\"To make predictions on new data:\")\n",
        "print(\"1. Preprocess the data exactly as you did for training\")\n",
        "print(\"2. Convert to tensor: torch.tensor(data, dtype=torch.float32).to(device)\")\n",
        "print(\"3. Make prediction: model(tensor).item()\")\n",
        "print(\"4. If you applied log transform to prices, remember to inverse transform!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "Congratulations! You've successfully built a PyTorch regression model for house price prediction.\n",
        "\n",
        "**What you learned:**\n",
        "1. How to structure a regression problem (continuous output vs. classification)\n",
        "2. The key differences in PyTorch implementation:\n",
        "   - Using MSE loss instead of BCE loss\n",
        "   - No sigmoid/softmax activation on the output layer\n",
        "   - Evaluating with regression metrics (MSE, RMSE, MAE, R²) instead of accuracy\n",
        "3. How to train, validate, and test a regression model\n",
        "4. How to visualize predictions and errors\n",
        "\n",
        "**To improve your model, you could try:**\n",
        "1. Adding more hidden layers or neurons\n",
        "2. Using different activation functions\n",
        "3. Implementing learning rate scheduling\n",
        "4. Adding dropout or batch normalization for regularization\n",
        "5. Trying different optimizers (AdamW, SGD with momentum)\n",
        "6. Feature engineering in the preprocessing stage\n",
        "7. Experimenting with different batch sizes and number of epochs\n",
        "8. Using early stopping to prevent overfitting\n",
        "\n",
        "Feel free to experiment with the hyperparameters and architecture!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
